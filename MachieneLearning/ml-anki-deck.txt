#separator:tab
#html:false
What is an API in the context of machine learning?	An Application Programming Interface (API) is a set of rules that allows different software programs to communicate with each other. Example: Like a restaurant menu that creates a standard interface between customers and the kitchen.
Define "classification" in machine learning.	Classification is a supervised learning technique that assigns input data into predefined categories or classes. Example: Sorting emails into "spam" or "not spam" based on their content.
What is a classifier in machine learning?	A classifier is an algorithm that maps input data to specific categories based on patterns it has learned. Example: Like a sorting hat that assigns students to different houses based on their characteristics.
What is cross-validation in machine learning?	Cross-validation is a technique to assess how well a model will generalize to independent data by partitioning the original data into training and testing subsets. Example: Like taking multiple practice exams with different questions to ensure you truly understand the material.
What do we call a table in machine learning where each row represents a different item and each column represents a detail about the item?	A data matrix or dataset. Example: A spreadsheet where each row is a different patient and each column is a different medical measurement.
What does early stopping mean in machine learning?	Early stopping is a regularization technique that halts model training when performance on a validation dataset begins to degrade, preventing overfitting. Example: Like stopping a cooking process at the optimal point before the food burns.
What is an estimator in machine learning?	An estimator is an algorithm or model that learns patterns from data to make predictions or decisions. Example: Like a weather forecaster that uses current conditions to predict tomorrow's weather.
What are features, variables, attributes, descriptors, and covariates in machine learning?	These terms all refer to the individual measurable properties or characteristics of the phenomenon being observed. Example: For a house price prediction model, features might include square footage, number of bedrooms, and neighborhood.
What does generalization performance mean in machine learning?	Generalization performance refers to how well a model performs on previously unseen data, rather than the data it was trained on. Example: Like being able to apply math skills to new types of problems, not just ones you've practiced.
What are hyperparameters in machine learning?	Hyperparameters are configuration settings for algorithms that are set before training begins and control the learning process. Example: Like setting the temperature and cooking time for a recipe before you start cooking.
What does infer or inference mean in machine learning?	Inference is the process of using a trained model to make predictions on new, unseen data. Example: Like using your knowledge of a person to predict how they'll react in a new situation.
What are learned parameters in machine learning?	Learned parameters are the internal values that a model adjusts during training to better fit the data, such as weights and biases in neural networks. Example: Like the muscle memory you develop after practicing a sport repeatedly.
What is a meta-estimator in machine learning?	A meta-estimator is an algorithm that takes other estimators as input and combines or enhances them to improve performance. Example: Like a coach who coordinates a team of specialists to achieve better results than any individual could.
What is a model in machine learning?	A model is a mathematical representation of a real-world process, learned from data, that can make predictions or decisions without being explicitly programmed. Example: Like a mental map that helps you navigate a city based on previous experiences.
What is overfitting in machine learning?	Overfitting occurs when a model learns the training data too precisely, including its noise and outliers, making it perform poorly on new data. Example: Like memorizing exam answers without understanding the underlying concepts, which fails when questions are rephrased.
What is a predictor in machine learning?	A predictor is a feature used by a model to predict a target variable. Example: Using recent rainfall (predictor) to forecast crop yield (target).
Define "predict" or "prediction" in machine learning.	Prediction is the process of a trained model estimating outputs for new inputs based on patterns learned from training data. Example: Like forecasting tomorrow's temperature based on historical weather patterns.
What is regression in machine learning?	Regression is a supervised learning technique that predicts continuous numerical values based on input features. Example: Predicting a house's price based on its size, location, and age.
What is a regressor in machine learning?	A regressor is an algorithm that estimates the relationships between dependent and independent variables to predict continuous numerical values. Example: An algorithm that predicts a car's fuel efficiency based on its weight and engine size.
What does regularization or penalization mean in machine learning?	Regularization is a technique that reduces model complexity by adding a penalty term to the loss function, preventing overfitting. Example: Like adding handicaps to certain moves in a game to prevent players from exploiting a dominant strategy.
What is a sample, instance, or observation in machine learning?	A sample, instance, or observation is a single row in a dataset representing one complete record. Example: In a medical dataset, one patient's entire record would be a single sample.
What is supervised learning in machine learning?	Supervised learning is an approach where models learn from labeled training data to map inputs to known outputs. Example: Learning to identify fruits after being shown many labeled examples of apples, oranges, and bananas.
What are target, label, and annotation in machine learning?	These terms refer to the known output values that supervised learning models try to predict. Example: In an email classifier, the labels would be "spam" or "not spam" for each email.
What is a test set in machine learning?	A test set is a portion of data kept separate from training that is used to evaluate a model's performance on unseen data. Example: Like a final exam that tests what you've learned from your study materials.
What does it mean to train, learn, or fit a model in machine learning?	Training a model means adjusting its parameters based on training data to minimize prediction errors. Example: Like practicing chess repeatedly to recognize patterns and improve your strategy.
What is a train set in machine learning?	A train set is the portion of data used to train a model by adjusting its parameters to fit patterns in the data. Example: Like study materials you use to prepare for an exam.
What is a transformer in machine learning?	A transformer is a component that applies mathematical operations to features to prepare them for modeling, such as scaling or encoding. Example: Like preparing ingredients before cooking by washing, cutting, or measuring them.
What is underfitting in machine learning?	Underfitting occurs when a model is too simple to capture the underlying patterns in the data, resulting in poor performance. Example: Like using only a linear equation to model a complex, curved relationship.
What is unsupervised learning in machine learning?	Unsupervised learning is an approach where models find patterns in unlabeled data without predefined outputs to predict. Example: Grouping customers into segments based on purchasing behavior without predefined categories.
What is a validation set in machine learning?	A validation set is a portion of data used to tune hyperparameters and evaluate model performance during training, before final testing. Example: Like a practice exam that helps you adjust your study strategy before the final exam.
In machine learning, what are commonly used equivalent terms for "row"?	"Sample", "record", "instance", or "observation". Each represents one complete example in your dataset.
In machine learning, what term is commonly used for each column in a dataset?	"Feature". Equivalent terms include "variable", "attribute", and "covariate". Each represents one type of measurement or characteristic.
How can you differentiate between numerical and categorical data?	Numerical data consists of continuous or discrete quantities (e.g., height, age), while categorical data represents distinct categories or groups (e.g., color, gender).
What is a "distribution" in simple terms?	A distribution shows how values of a feature are spread out, showing which values occur frequently and which are rare. Example: Like a histogram showing how many students got each possible score on a test.
What are categorical features?	Categorical features represent distinct groups with no inherent order or numerical relationship. Example: Blood type (A, B, AB, O) or product category (electronics, clothing, food).
What are numerical features?	Numerical features represent quantitative measurements that can be discrete (countable) or continuous (measurable). Example: Age (25 years) or temperature (98.6Â°F).
Why is it important to distinguish between feature types?	Distinguishing feature types helps select appropriate preprocessing techniques and algorithms, as different models handle different data types with varying effectiveness.
What does the term "tabular dataset" refer to?	A tabular dataset is structured data organized in rows and columns, like a spreadsheet, where rows represent individual observations and columns represent features.
What are discrete variables?	Discrete variables take on countable, distinct values with clear separations between them. Example: Number of children in a family (0, 1, 2, 3, etc.).
What are continuous variables?	Continuous variables can take any value within a range and are typically measured, not counted. Example: Height, weight, time, or temperature.
What is the meaning of "mean" in machine learning?	The mean is the average value of a numeric feature, calculated by summing all values and dividing by the count of values. It represents the central tendency of the data.
What is labeled data?	Labeled data includes both input features and corresponding output targets, enabling supervised learning algorithms to learn the relationship between them. Example: Images with labels identifying what object each contains.
What is unlabeled data?	Unlabeled data contains only input features without corresponding output targets, commonly used in unsupervised learning to discover patterns. Example: Customer transaction data without predefined segments.
What's the difference between generalization performance and cross-validation?	Generalization performance is a model's ability to perform well on unseen data, while cross-validation is a technique to estimate that performance by systematically partitioning data into multiple training and testing subsets.
What is feature scaling in machine learning?	Feature scaling is the process of standardizing the range of features to ensure each contributes equally to the model, preventing features with larger scales from dominating. Example: Converting heights (170-190cm) and weights (50-100kg) to comparable scales.
Why is feature scaling important for distance-based models?	Feature scaling prevents features with larger scales from dominating distance calculations, ensuring each feature contributes proportionately to similarity measures. Example: Without scaling, a difference of 1kg in weight would be considered less significant than a 1cm difference in height.
Which machine learning models benefit most from feature scaling?	Models that use distance calculations (k-nearest neighbors, k-means) or gradient-based optimization (neural networks, support vector machines, logistic regression) benefit most from feature scaling.
What does StandardScaler from scikit-learn do?	StandardScaler transforms features to have zero mean and unit variance by subtracting the mean and dividing by the standard deviation, creating standardized features with comparable scales.
Explain the two steps in using StandardScaler for feature scaling.	The two steps are: 1) Fit - calculate the mean and standard deviation of each feature; 2) Transform - adjust each feature by subtracting its mean and dividing by its standard deviation.
What does "zero mean" mean in data scaling?	"Zero mean" means shifting data so the average value of each feature becomes zero, creating a centered distribution by subtracting the original mean from each value.
What does "unit variance" mean in feature scaling?	"Unit variance" means scaling data so each feature has a standard deviation of one, achieved by dividing each value by the feature's standard deviation, ensuring consistent spread across features.
How does a validation set help tune hyperparameters?	A validation set helps tune hyperparameters by providing an unbiased evaluation of models with different hyperparameter settings. For each configuration, you train on the training set, evaluate on the validation set, and select the hyperparameters that perform best on the validation data.
What is the bias-variance tradeoff in machine learning?	The bias-variance tradeoff represents the balance between a model's ability to fit the training data (low bias) and its ability to generalize to new data (low variance). A simple model may have high bias but low variance, while a complex model may have low bias but high variance.
What is a confusion matrix in classification?	A confusion matrix is a table showing the counts of true positives, false positives, true negatives, and false negatives from a classification model, helping evaluate performance across different classes. Example: Showing how many emails were correctly or incorrectly classified as spam or not spam.
What is precision in classification metrics?	Precision measures the proportion of positive predictions that were actually correct. It answers: "Of all items labeled as positive, how many were truly positive?" Example: Of all emails classified as spam, what percentage were actually spam.
What is recall in classification metrics?	Recall measures the proportion of actual positives that were correctly identified. It answers: "Of all actual positive items, how many did we catch?" Example: Of all actual spam emails, what percentage did we correctly identify as spam.
What is the F1 score in classification metrics?	The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. It's especially useful when classes are imbalanced. Example: A balanced measure of how well a spam filter catches spam while avoiding false positives.
What is a neural network in machine learning?	A neural network is a model inspired by the human brain's structure, consisting of interconnected nodes (neurons) organized in layers that can learn complex patterns from data. Example: Like a complex circuit that can recognize images by learning patterns of pixels.
What is gradient descent in machine learning?	Gradient descent is an optimization algorithm that iteratively adjusts model parameters to minimize error by moving in the direction of steepest descent of the error surface. Example: Like finding the lowest point in a valley by always stepping downhill.
What is a decision tree in machine learning?	A decision tree is a flowchart-like model that makes decisions by following a path from the root node to a leaf node based on feature values. Example: Like a series of yes/no questions that lead to a final decision.
What is random forest in machine learning?	Random forest is an ensemble method that combines multiple decision trees trained on different subsets of data to produce a more accurate and stable prediction. Example: Like consulting a committee of experts rather than relying on just one opinion.
What is k-means clustering in machine learning?	K-means clustering is an unsupervised algorithm that partitions data into k distinct clusters based on feature similarity by minimizing the distance between points and their assigned cluster centers. Example: Like organizing books on shelves based on their topics.
What is the purpose of training data in machine learning?	Training data (60-80% of dataset) is used to learn the model's core parameters (weights, coefficients). The model accesses both inputs and correct answers to minimize prediction errors. Like studying course material to understand the subject.
What is the purpose of validation data in machine learning?	Validation data (10-20% of dataset) is used to tune hyperparameters and monitor for overfitting. Provides feedback to adjust model configuration without contaminating the test set. Like a practice exam that helps adjust study strategy.
What is the purpose of test data in machine learning?	Test data (10-20% of dataset) provides final, unbiased evaluation of model performance on unseen data. Never used for training or tuning. Like a final exam that truly measures learned knowledge.
